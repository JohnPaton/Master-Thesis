\chapter{Implementation}
\label{chap:Implementation}
In this chapter we explain the infrastructure that performs all the necessary steps to produce an efficient feedback. A general overview is given and for each section, we describe in particular the tools as well as the way we manipulated the data in order to obtain the information useful for the user. The chapter is divided in two parts: the first part focuses on the back-end and the services we used to extract the features we described in \ref{chap:Speech Recognition}. The second part describe the front-end, that is, the \textit{Android}\footnote{\url{https://www.android.com}} application (called \textbf{PARLA}\footnote{\url{https://github.com/davideberdin/PARLA}}) with a particular focus on the feedback page and the general usage.

\section{General architecture}
\label{sec:general_architecture}

In \ref{fig:general_architecture} is shown the general architecture of the infrastructure.
The flow displays only the \textit{pronunciation testing} phase:

\begin{itemize}
	\item[1)] User says the sentence using the internal microphone of the smartphone (or through the headset)
	\item[2)] The application sends the audio file to the \textit{Speech Recognition service}
	\item[3)] The result of step 2 is sent to the \textit{Gaussian Mixture Model service}
	\item[4)] The result of step 3 is sent back to the application where a \textit{Feedback page} is displayed
	\item[5)] A short explanation for each chart is given to the user
	\item[6)] Back to step 1
\end{itemize}

\noindent The flow described above is the main feature of the whole project. Although, the application supplies other two important functionalities that are described more in detail in \ref{sec:android_app}. The first one is related to \textbf{critical listening} where the user is able to listen to the \textit{Native pronunciation} as well as to its one. This feature have a big impact on improving the pronunciation because it pushes the user to understand the differences as well as to emulate the way native speakers pronounce a specific sequence of words. The second feature regards the \textbf{history} (or progress). This page shows the trend of the user based on all the pronunciation he/she made during the usage of PARLA. The purpose of the history page is to help the user to see the progresses and to get an idea of how to improve the pronunciation. \\

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.6]{Figures/general_architecture.png}
	\caption{General architecture of the infrastructure}
	\label{fig:general_architecture}
\end{figure}

\subsubsection{Implementation procedure}
\label{ssec:procedure}

Several step were made before to reach the architecture depicted in \ref{fig:general_architecture}. Generally speaking, we divided the implementation in two main categories: the first is composed by the \textit{data collection and training} phase whereas the second is formed by the \textit{mobile application} and \textit{server communication}. \\

\noindent The very first step was to collect the data from native speakers and apply some pre-processing techniques in such way that we were able to obtain only the information we needed to train the two services we had on the server.After the data collection, we trained both the models with the information we extracted in the previous step. The detailed procedures are described in \ref{ssec:training_sr_model} and \ref{ssec:training_gmm}. \\
\noindent When the training phase was completed, we set up the services and used \textit{REST} calls to communicate with the mobile application. These two parts were developed at the same time and are described in detail in \ref{sec:android_app}.


\section{Data collection}
\label{sec:data_collection}

The data collection step is a crucial phase of the entire project. The reason for such importance is that the audio record has to be clear, clean and as natural as possible. In fact, the people who participate to this phase were asked to pronounce the sentences as they would say in a day-by-day conversation. \\

\noindent We recorded 8 people divided in 4 males and 4 females at the University of Rochester using \textit{Audacity}\footnote{\url{http://audacityteam.org}}. Each person had to pronounce 10 sentences (see \ref{table:sentences}) and each sentence was pronounced 10 times. \\
\noindent The sentences were chosen in order to cover the most used English sounds and based on the frequencies of everyday usage\footnote{\url{http://www.learn-english-today.com/idioms/idioms_proverbs.html}}. 

\begin{table}[!ht]
	\centering
	\begin{tabular}{|c|c|}
		\hline
		\multicolumn{2}{|c|}{Sentences}      \\ \hline
		A piece of cake  & Fair and square   \\ \hline
		Blow a fuse      & Get cold feet     \\ \hline
		Catch some zs    & Mellow out        \\ \hline
		Down to the wire & Pulling your leg  \\ \hline
		Eager beaver     & Thinking out loud \\ \hline
	\end{tabular}
	\caption{Idioms used for testing the pronunciation}
	\label{table:sentences}
\end{table}

\noindent The total file we gathered were 800 and the average length of each file is \textbf{1s}. In total, we were able to gather 14 minutes of recorded audio. This amount of time was sufficient for training the speech recognition model and the GMM. In reality, for the speech recognition service, we initially trained the model with a bigger dataset and then we added the one with the sentences (details in \ref{fig:sphinx_service}). The reason is that the tool we used for the speech recognition, requires a large dataset\footnote{\url{http://cmusphinx.sourceforge.net/wiki/tutorialam}}.

\subsection{Data pre-processing}
\label{ssec:pre_processing}

The data pre-processing step is one of the most important procedure of the whole project. In fact, extracting the right information is crucial for both training the models and those voice-features that should be showed to the user. \\

\noindent The process starts by using the tool called \textbf{PRAAT}\footnote{http://www.fon.hum.uva.nl/praat/}. This tool is used for \textit{analysis of speech in phonetics} as well as for \textit{speech synthesis} and\textit{articulatory synthesis} \cite{boersma2001praat}. With PRAAT, we were able to analyze the audio files we collected in the very beginning of the project and extracting formants and stress. In \ref{sub:formants} and \ref{sec:stress} we detailed explained the meaning of these parameters. \\
From here, we generated a set of \textit{CSV} files where we saved the values of the formants and the stress for each audio file. These file are then used as input for a tool called \textbf{FAVE-Align}\cite{yuan2008speaker}. \\

\noindent FAVE-Align is a tool used for \textit{force alignment}. This process is used to determine where a particular word occurs in an audio frame \cite{forced_alignment_def}. In other words, FAVE-Align takes a text transcription and produce a PRAAT TextGrid file where it shows when those words start and end in the related audio file. In \ref{fig:fave-align_result}, there is an example of this procedure. The tool performs different phases in order to align audio and text.\\
The first step is to sample the audio file and apply the Fourier Transformation because there is the need to move from the \textit{time domain} to \textit{frequencies domain}. From here, the tool extract the \textit{spectrum} and apply the Inverse Fourier Transformation on it to obtain the so called \textbf{Cepstrum}. The \textit{cepstrum} is the representation in a small-window frame of the spectrum. Although, the amount of information extracted from the cepstrum are too high, and for this reason, the tool uses \textit{Perceptual Linear Prediction coefficients} to retrieve the necessary data to perform the alignment decision. These coefficients are used for feature extraction. The detailed process can be found at \cite{hermansky1990perceptual}. \\
The last part of this process is the decision making part and this is done by a \textit{Hidden Markov Model}. \\ 

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.5]{Figures/fave_align.png}
	\caption{Result from FAVE-Align tool opened in PRAAT}
	\label{fig:fave-align_result}
\end{figure}

\noindent The outcome of the previous step is used as input for the tool called \textbf{FAVE-Extract}. This tool helps to automates the vowel formant analysis. The process is dived in two main steps: the first is finding the \textit{Measurement Points} and the second is the \textit{Remeasurement}. \\
In \cite{rosenfelder2011fave} is explained that for most vowels it is possible to find the measurement point by listening 1/3 of the total duration. This point is necessary for determining the identity of the vowel, that is, the name of the vowel itself. For more complex vowels, a different approach is done, that is, the point is halfway between the F1 (main formant) maximum value and the beginning of the segment. In addition, the LPC analysis is performed on both beginning and end of the vowel in order to pad the vowel's window. This \emph{ensure a formant track through the full vowelâ€™s duration}\cite{harrison2004variability}. The result of this step is a set of candidates. This set is composed by the potential formants estimated from the likelihood of the \textbf{ANAE} distribution. The \textit{Atlas of North American English} (ANAE) is the set of phonology formants values depending on the English regional area. The winner formant is determined by the Posterior probability. This step does not take in consideration the provenience of the speaker. \\

\noindent The second part of the formants extraction tool is to remeasure the parameters by adjusting the ANAE distribution based on the regional area of the speaker. In this way, the formant value will be more accurate. An example of result from FAVE-Extract is shown in \ref{fig:fave-extract_example}.

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.5]{Figures/fave-extract_example.png}
	\caption{Result from FAVE-Extract}
	\label{fig:fave-extract_example}
\end{figure}

\noindent The result of the data pre-processing is a set of information composed by the average value of F1 and F2 formants with their respectively vowels text representation. The formants values will be then used to train both the speech recognition model and the Gaussian Mixture Model.

\section{Server}
\label{sec:server}

The back-end system is divided in two different services: the first one handles the speech recognition converting the user's voice into a set of phonemes, whereas the second service is in charged of all the other operations a user can do, such as login/logout, history data, vowels prediction system, usage collection, etc.. This section explains more in detail how we extract the information from the audio files and how we manipulate those before to give the feedback to the user.

\subsection{Training the Speech Recognition model} 
\label{ssec:training_sr_model}

goofy

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.6]{Figures/sphinx_service.png}
	\caption{Architecture of the Speech recognition service}
	\label{fig:sphinx_service}
\end{figure}

\subsection{Training the Gaussian Mixture Model}
\label{ssec:training_gmm}

goofy

\begin{figure}[!ht]
	\centering
	\includegraphics[scale=0.6]{Figures/gmm_service.png}
	\caption{Architecture of the Gaussian Mixture Model service}
	\label{fig:gmm_service}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%							ANDROID PART							   %%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Android application}
\label{sec:android_app}

\subsection{Layouts}
\label{ssec:layouts}

\subsection{Feedback layout}
\label{ssec:feedback_layout}

\subsection{Usage procedure}
\label{ssec:usage_procedure}